{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9181cf2e-e782-4238-acff-b7cf35b70924",
   "metadata": {},
   "source": [
    "# FOR TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4e510-82f5-472b-bd04-dba95d565ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import mne\n",
    "import os, shutil, fnmatch\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c736d9ce-7489-44c3-8b47-ba48e4288251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_num_of_file(dir):\n",
    "\n",
    "    print(f'======== The process of finding the optimal number of images for each stage has been started. ========')\n",
    "    print(f'======== During the process files of the supplied directory \"dir\" will be analysed. ========\\n')\n",
    "\n",
    "    all_files_in_dir = os.listdir(dir)\n",
    "    stages = 6*[0]\n",
    "\n",
    "    all_psg_files = []\n",
    "    all_hyp_files = []\n",
    "    checked = 0\n",
    "    annotation_desc_2_event_id = {\n",
    "        \"Sleep stage W\": 1,\n",
    "        \"Sleep stage 1\": 2,\n",
    "        \"Sleep stage 2\": 3,\n",
    "        \"Sleep stage 3\": 4,\n",
    "        \"Sleep stage 4\": 5,\n",
    "        \"Sleep stage R\": 6,\n",
    "    }\n",
    "\n",
    "    for file in all_files_in_dir:\n",
    "        if file[0] == '.': # for ignoring hidden files in the directory\n",
    "            continue\n",
    "        parts = file.split(\"-\")\n",
    "        if parts[1] == \"PSG.edf\":\n",
    "            all_psg_files.append(file)\n",
    "        elif parts[1] == \"Hypnogram.edf\":\n",
    "            all_hyp_files.append(file)\n",
    "\n",
    "    for file in all_psg_files:\n",
    "\n",
    "        current_psg_file = dir + \"/\" + file\n",
    "\n",
    "        hyp_file = file.split(\"-\")[0][:-2] + \"*\" + \"-Hypnogram.edf\"\n",
    "        possible_hyp_file = fnmatch.filter(all_hyp_files, hyp_file)\n",
    "\n",
    "        if possible_hyp_file:\n",
    "\n",
    "            current_hyp_file = dir + \"/\" + possible_hyp_file[0]\n",
    "\n",
    "            data = mne.io.read_raw_edf(current_psg_file, stim_channel=\"Event marker\", infer_types=True, preload=True, verbose=False)\n",
    "\n",
    "            annot_train = mne.read_annotations(current_hyp_file)\n",
    "\n",
    "            annot_train.crop(annot_train[1][\"onset\"] - 30*60, annot_train[-2][\"onset\"] + 30*60)\n",
    "            data.set_annotations(annot_train, emit_warning=False)\n",
    "\n",
    "            events_from_the_file, _ = mne.events_from_annotations(\n",
    "                data, event_id=annotation_desc_2_event_id, chunk_duration=30.0, verbose=False\n",
    "            )\n",
    "\n",
    "            for event in events_from_the_file:\n",
    "                stages[list(annotation_desc_2_event_id.values()).index(event[2])] += 1\n",
    "\n",
    "            checked += 1\n",
    "            print(f\"======== Files have been analysed: {checked}/{len(all_psg_files)} ========\", end=\"\\r\")\n",
    "        else:\n",
    "            print(f\"\\n======== No such hypnogram file for {file} ========\\n\")\n",
    "\n",
    "    print(\"\\n======== Analysing is ended. ========\\n\")\n",
    "    i = 0\n",
    "    for stage in list(annotation_desc_2_event_id):\n",
    "        print(f\"======== Number of files for {stage} is {stages[i]} ========\")\n",
    "        i += 1\n",
    "\n",
    "    print(f\"======== Minimum number of the files for each stage is {min(stages)} ========\")\n",
    "\n",
    "    return min(stages), max(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc4e2cb-cc2c-4bc7-94f4-b940a15ffc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_keeper(psg_file_path, hyp_file_path):\n",
    "\n",
    "    data = mne.io.read_raw_edf(psg_file_path, stim_channel=\"Event marker\", infer_types=True, preload=True)\n",
    "    annotations = mne.read_annotations(hyp_file_path)\n",
    "\n",
    "    data.filter(0.5, 30, picks=[0])\n",
    "\n",
    "    return data, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1964db5-0ed6-4ffc-b6be-78d14b72f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_set_annotations(data, annotations):\n",
    "\n",
    "    annotations.crop(annotations[1][\"onset\"] - 30*60, annotations[-2][\"onset\"] + 30*60)\n",
    "    data.set_annotations(annotations, emit_warning=False)\n",
    "\n",
    "    annotations_stage_id = {\n",
    "        \"Sleep stage W\": 1,\n",
    "        \"Sleep stage 1\": 2,\n",
    "        \"Sleep stage 2\": 3,\n",
    "        \"Sleep stage 3\": 4,\n",
    "        \"Sleep stage 4\": 5,\n",
    "        \"Sleep stage R\": 6,\n",
    "    }\n",
    "\n",
    "    events_from_the_file, event_id_info = mne.events_from_annotations(\n",
    "        data, event_id= annotations_stage_id, chunk_duration= 30.0\n",
    "    )\n",
    "\n",
    "    return events_from_the_file, event_id_info, annotations_stage_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c6fb73-bbaa-471e-ae81-99e9eecc30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "\n",
    "    directories = ()\n",
    "    dir_name = \"dataset\"\n",
    "\n",
    "    sleep_stages = {\n",
    "        \"Sleep stage W\": 1,\n",
    "        \"Sleep stage 1\": 2,\n",
    "        \"Sleep stage 2\": 3,\n",
    "        \"Sleep stage 3\": 4,\n",
    "        \"Sleep stage 4\": 5,\n",
    "        \"Sleep stage R\": 6,\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(dir_name):\n",
    "\n",
    "        main_dir = dir_name\n",
    "        train_dir = dir_name + \"/train\"\n",
    "        test_dir = dir_name + \"/test\"\n",
    "        valid_dir = dir_name + \"/valid\"\n",
    "\n",
    "        train_files_dir = []\n",
    "        test_files_dir = []\n",
    "        valid_files_dir = []\n",
    "\n",
    "        for i in sleep_stages:\n",
    "            if i == 'Sleep stage ?':\n",
    "                pass\n",
    "            else:\n",
    "                train_files_dir.append(dir_name + \"/train/\" + i)\n",
    "                test_files_dir.append(dir_name + \"/test/\" + i)\n",
    "                valid_files_dir.append(dir_name + \"/valid/\" + i)\n",
    "    \n",
    "        directories = (main_dir, train_dir, train_files_dir, test_dir, test_files_dir, valid_dir, valid_files_dir )\n",
    "    \n",
    "        for directory in directories:\n",
    "            if isinstance(directory, list):\n",
    "                for sub_directory in directory:\n",
    "                    if os.path.isdir(sub_directory):\n",
    "                        pass\n",
    "                    else:\n",
    "                        os.mkdir(sub_directory)\n",
    "    \n",
    "            else:\n",
    "                if os.path.isdir(directory):\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(directory)\n",
    "    \n",
    "        print(\"======== Directories have been created. ========\")\n",
    "    else:\n",
    "        print(\"Dataset directory already exists.\")\n",
    "        raise SystemExit(\"Please, remove old dataset directory!\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed4f4df-337d-4a30-ad10-5b520b5096b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images(data, events_from_the_file, event_id_info, annotations_stage_id, psg_file, optim_num_imgs, ctrl_imgs, place_for_image):\n",
    "\n",
    "    tmax = 30.0 - 1.0 / data.info[\"sfreq\"]\n",
    "    print(f\"Number of epochs in the events_from_the_file for the file: {len(events_from_the_file)}, number of images to be created: {optim_num_imgs}\")\n",
    "\n",
    "    epochs_from_the_file = mne.Epochs(\n",
    "        raw=data,\n",
    "        events=events_from_the_file,\n",
    "        event_id=event_id_info,\n",
    "        tmin=0.0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    events = epochs_from_the_file.get_data(picks=[0])\n",
    "    times_events = epochs_from_the_file.events\n",
    "\n",
    "    for iteration in range(len(events)):\n",
    "        \n",
    "        start_time = int(times_events[iteration][0] / data.info.get('sfreq'))   # start moment of the signal part on the graph\n",
    "\n",
    "        if ctrl_imgs[times_events[iteration][2] - 1] < optim_num_imgs:\n",
    "\n",
    "            plt.ioff()\n",
    "\n",
    "            fig = figure.Figure(figsize=(8,8))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            ax.plot(events[iteration][0], linewidth=0.5, color=\"black\")\n",
    "\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            stage_list = list(annotations_stage_id.keys())\n",
    "            stage_id = list(annotations_stage_id.values()).index(times_events[iteration][2])\n",
    "            path_for_image = f\"dataset/{place_for_image}/\" + stage_list[stage_id] + \"/\" + stage_list[stage_id] + \\\n",
    "                    '_' + psg_file.split(\"-\")[0] + '_' + str(start_time)\n",
    "\n",
    "            fig.savefig(path_for_image, bbox_inches='tight')\n",
    "\n",
    "            plt.close(fig)\n",
    "\n",
    "            ctrl_imgs[times_events[iteration][2] - 1] += 1\n",
    "\n",
    "    print(f\"\\nImages have been prepared for {psg_file.split('-')[0][0:-2]}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28491d8-c4e1-4f18-919d-c7b91d77075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_create(files_directory, optim_num_imgs, place_for_image):\n",
    "\n",
    "    all_files = os.listdir(files_directory)\n",
    "    not_processed_files = []\n",
    "    all_psg_files = []\n",
    "    all_hyp_files = []\n",
    "\n",
    "    ctrl_imgs = 6*[0] # List for the number of images (for 6 stages)\n",
    "\n",
    "    for file in all_files:\n",
    "\n",
    "        if file[0] == '.': # for ignoring hidden files in the directory\n",
    "            continue\n",
    "\n",
    "        parts = file.split(\"-\")\n",
    "\n",
    "        if parts[1] == \"PSG.edf\":\n",
    "            all_psg_files.append(file)\n",
    "        elif parts[1] == \"Hypnogram.edf\":\n",
    "            all_hyp_files.append(file)\n",
    "\n",
    "    # for controlling status progress\n",
    "    old_process_percentage = 0\n",
    "    process_percentage = 0\n",
    "    iteration = 0\n",
    "\n",
    "    for psg_file in all_psg_files:\n",
    "\n",
    "        hyp_file = psg_file.split(\"-\")[0][:-2] + \"*\" + \"-Hypnogram.edf\"\n",
    "        possible_hyp = fnmatch.filter(all_hyp_files, hyp_file)\n",
    "\n",
    "        if possible_hyp:\n",
    "\n",
    "            hyp_file = possible_hyp[0]\n",
    "\n",
    "            print(f\"\\n================ Files currently being processed: {psg_file}, {hyp_file} ================\")\n",
    "\n",
    "            psg_file_path = files_directory + \"/\" + psg_file\n",
    "            hyp_file_path = files_directory + \"/\" + hyp_file\n",
    "\n",
    "            data, annotations = file_keeper(psg_file_path, hyp_file_path)\n",
    "            print(\"======== Got data and annotations. ========\")\n",
    "\n",
    "            events_from_the_file, event_id_info, annotations_stage_id = crop_set_annotations(data, annotations)\n",
    "            print(\"======== Annotations cropped and set. ========\")\n",
    "\n",
    "            create_images(data, events_from_the_file, event_id_info, annotations_stage_id, psg_file, optim_num_imgs, ctrl_imgs, place_for_image)\n",
    "\n",
    "        else:\n",
    "            not_processed_file = psg_file.split(\"-\")[0][:-2] # Get number of the candidate, i.e. SC4812\n",
    "            print(f\"No such hypnogram file for {not_processed_file}\")\n",
    "            not_processed_files.append(not_processed_file)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        process_percentage = round(iteration / len(all_psg_files) * 100)  # progress controlling\n",
    "\n",
    "        if process_percentage != old_process_percentage:\n",
    "            print(f\"======== Extracting images from PSG signals: {process_percentage}% ========\", end=\"\\r\")\n",
    "\n",
    "        old_process_percentage = process_percentage\n",
    "\n",
    "    print(\"\\nEND. Images for the dataset have been created.\")\n",
    "    if not_processed_files:\n",
    "        print(f\"Files that weren't processed: {not_processed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391321d-2390-4a12-8c2e-e44f4766473c",
   "metadata": {},
   "source": [
    "## FOR PREDICTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ead362-c1ee-4543-8bcf-6ac70fba9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_and_img_pred(name_predict_sig, data, events_from_the_file, event_id_info, annotations_stage_id, psg_file):\n",
    "\n",
    "    # Creating a directory\n",
    "\n",
    "    dir_name = \"dataset_predict\"\n",
    "\n",
    "    name_predict_sig = dir_name + \"/\" + name_predict_sig    # creating  a full path of the folder\n",
    "\n",
    "    directories = (dir_name, name_predict_sig)\n",
    "\n",
    "    for directory in directories:\n",
    "        if os.path.isdir(directory):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(directory)\n",
    "            print(f\"======== Directory {directory} has been created. ========\")\n",
    "\n",
    "    # Creating an image\n",
    "\n",
    "    tmax = 30.0 - 1.0 / data.info[\"sfreq\"]\n",
    "    \n",
    "    epochs_from_the_file = mne.Epochs(\n",
    "        raw=data,\n",
    "        events=events_from_the_file,\n",
    "        event_id=event_id_info,\n",
    "        tmin=0.0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    events = epochs_from_the_file.get_data(picks=[0])\n",
    "    times_events = epochs_from_the_file.events\n",
    "\n",
    "    i = 0\n",
    "    old_process_percntage = 0\n",
    "    process_percentage = 0\n",
    "\n",
    "    for iteration in range(len(events)):\n",
    "        \n",
    "        process_percentage = round(i / len(events) * 100)    # process controlling\n",
    "        if process_percentage != old_process_percntage:\n",
    "            print(f\"======== Extracting images from PSG signals: {process_percentage}% ========\", end=\"\\r\")\n",
    "\n",
    "        start_time = int(times_events[iteration][0] / data.info.get('sfreq'))   # start moment of the signal part on the graph\n",
    "\n",
    "        plt.ioff()\n",
    "\n",
    "        fig = figure.Figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        ax.plot(events[iteration][0], linewidth=0.5, color=\"black\")\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        stage_list = list(annotations_stage_id.keys())\n",
    "        stage_id = list(annotations_stage_id.values()).index(times_events[iteration][2])\n",
    "\n",
    "        path_for_image = name_predict_sig + '/' + stage_list[stage_id] + \\\n",
    "                        '_' + psg_file.split(\"-\")[0] + '_' + str(start_time)\n",
    "\n",
    "        fig.savefig(path_for_image, bbox_inches='tight')\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "        old_process_percentage = process_percentage\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f\"\\n Images have been preapred for {psg_file.split('-')[0][0:-2]}.\\n\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9bfabcf-c8d0-4fe1-ad63-116469745c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_dataset(files_directory):\n",
    "\n",
    "    print(f\"======== The process of predict dataset has been started. ========\")\n",
    "    print(f\"======== During the process files of the supplied directory will be processed. ========\\n\")\n",
    "\n",
    "    all_files = os.listdir(files_directory)\n",
    "    not_processed_files = []\n",
    "    all_psg_files = []\n",
    "    all_hyp_files = []\n",
    "\n",
    "    for file in all_files:\n",
    "        if file[0] == '.': # for ignoring hidden files\n",
    "            continue\n",
    "        parts = file.split(\"-\")\n",
    "        if parts[1] == \"PSG.edf\":\n",
    "            all_psg_files.append(file)\n",
    "        elif parts[1] == \"Hypnogram.edf\":\n",
    "            all_hyp_files.append(file)\n",
    "\n",
    "    old_process_percentage = 0\n",
    "    process_percentage = 0\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    for psg_file in all_psg_files:\n",
    "\n",
    "        process_percentage = round(iteration / len(all_psg_files) * 100) # process controlling\n",
    "        if process_percentage != old_process_percentage:\n",
    "            print(f\"\\n======== Extracting images from PSG signals: {process_percentage}% ========\\n\")\n",
    "\n",
    "        hyp_file = psg_file.split(\"-\")[0][:-2] + \"*\" + \"-Hypnogram.edf\"\n",
    "        possible_hyp = fnmatch.filter(all_hyp_files, hyp_file)\n",
    "\n",
    "        if possible_hyp:\n",
    "            hyp_file = possible_hyp[0]\n",
    "\n",
    "            print(f\"======== Files currently being processed: {psg_file}, {hyp_file} ========\")\n",
    "\n",
    "            psg_file_path = files_directory + \"/\" + psg_file\n",
    "            hyp_file_path = files_directory + \"/\" + hyp_file\n",
    "\n",
    "            data, annotations = file_keeper(psg_file_path, hyp_file_path)\n",
    "            print(\"======== Got data and annotations. ========\")\n",
    "\n",
    "            events_from_the_file, event_id_info, annotations_stage_id = crop_set_annotations(data, annotations)\n",
    "            print(\"======== Annotations cropped and set. ========\")\n",
    "\n",
    "            name_predict_sig = psg_file.split(\"-\")[0][:-2]\n",
    "            create_dir_and_img_pred(name_predict_sig, data, events_from_the_file, event_id_info, annotations_stage_id, psg_file)\n",
    "\n",
    "        else:\n",
    "            not_processed_file = psg_file.split(\"-\")[0][:-2] # Gets number of the candidate, i.e. SC4812\n",
    "            print(f\"No such hypnogram file for {not_processed_file}\")\n",
    "            not_processed_files.append(not_processed_file)\n",
    "\n",
    "        old_process_percentage = process_percentage\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"\\n======== Extracting images from PSG signals: {process_percentage}% ========\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n======== END. Images for the dataset have been created. ========\")\n",
    "\n",
    "    if not_processed_files:\n",
    "        print(f\"Files that weren't processed: {not_processed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecd009-7752-4951-b413-4c6a8ec5fc77",
   "metadata": {},
   "source": [
    "# FOR MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f435e9-63c5-4bdb-9736-3955940de3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7337e893-5d3a-4712-8744-b83174365c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_func():\n",
    "\n",
    "    train_transform = ImageDataGenerator(rescale=1./255,\n",
    "                                        shear_range=0.2,\n",
    "                                        zoom_range=0.2,\n",
    "                                        horizontal_flip=True)\n",
    "    training_set = train_transform.flow_from_directory('dataset/train',\n",
    "                                                      target_size=(800,800),\n",
    "                                                      color_mode=\"grayscale\",\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode='categorical')\n",
    "\n",
    "    \n",
    "    validation_transform = ImageDataGenerator(rescale=1./255)\n",
    "    validation_set = validation_transform.flow_from_directory('dataset/valid',\n",
    "                                                      target_size=(800,800),\n",
    "                                                      color_mode=\"grayscale\",\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode='categorical')\n",
    "\n",
    "\n",
    "    test_transform = ImageDataGenerator(rescale=1./255)\n",
    "    test_set = test_transform.flow_from_directory('dataset/test',\n",
    "                                                      target_size=(800,800),\n",
    "                                                      color_mode=\"grayscale\",\n",
    "                                                      batch_size=32,\n",
    "                                                      class_mode='categorical')\n",
    "\n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148644e-2a76-4672-9a55-fef0348e21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_cnn():\n",
    "\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(16, (3,3), activation='relu', input_shape=[800, 800, 1]))\n",
    "    cnn.add(MaxPooling2D((2,2)))\n",
    "    \n",
    "    cnn.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    cnn.add(MaxPooling2D((2,2)))\n",
    "\n",
    "    cnn.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    cnn.add(MaxPooling2D((2,2)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    cnn.add(Dense(64, activation='relu'))\n",
    "    cnn.add(Dense(units=6, activation='softmax'))\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cffe03-20a7-43d7-874c-bc071bd47594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_train_model(cnn, training_set, validation_set):\n",
    "\n",
    "    cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    cnn.fit(x=training_set, validation_data=validation_set, epochs=5)\n",
    "    if os.path.isdir('models'):\n",
    "        print(f\"Folder 'models' already exists.\")\n",
    "        cnn.save('models/exit_model.keras')\n",
    "    else:\n",
    "        os.mkdir('models')\n",
    "        cnn.save('models/exit_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8983e44-e9ce-4b69-9139-ff18b2ce2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_hid_files():   # for removing \".\" files\n",
    "\n",
    "    print(\"======== Checking on hidden '.' files started. ========\")\n",
    "\n",
    "    datasets_for_check = [\"dataset\", \"dataset_predict\"]\n",
    "\n",
    "    for dataset in datasets_for_check:\n",
    "        if not os.path.isdir(dataset):\n",
    "            print(f\"Directory '{dataset}' doesn't exist.\")\n",
    "            continue\n",
    "\n",
    "        for folder in os.listdir(dataset):\n",
    "            dir_1 = dataset + \"/\" + folder\n",
    "            if folder[0] == \".\":\n",
    "                os.remove(dir_1)\n",
    "\n",
    "            for folder_el in os.listdir(dir_1):\n",
    "                dir_2 = dir_1 + \"/\" + folder_el\n",
    "                if folder_el[0] == \".\":\n",
    "                    os.remove(dir_2)\n",
    "                if dataset == \"dataset\":\n",
    "                    for element in os.listdir(dir_2):\n",
    "                        if element[0] == \".\":\n",
    "                            dir_rem = dir_2 + \"/\" + element\n",
    "                            os.remove(dir_rem)\n",
    "\n",
    "        if os.path.isdir(dataset):\n",
    "            print(f\"======== Dataset '{dataset}' has been checked for hidden files. ========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889ef52-14b6-453b-ba9b-2ff1c1cc003b",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc4571-2711-4a5a-8901-50af3438567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hypnograms(events, times_events, sfreq, img_folder_path, cnn, annotation_stage_id, training_set):\n",
    "\n",
    "    # Coordinates for true hypnogram and predicted respectively\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    x_pred = []\n",
    "    y_pred = []\n",
    "\n",
    "    percent = 0\n",
    "    correct_predict = 0\n",
    "    true_list = 6*[0] # List for true numbers of the each stage, based on the annotation_stage_id\n",
    "    correct_predict_list = 6*[0] # List of the correct predictions for the each stage, based on the annotation_stage_id\n",
    "\n",
    "    stages_names_list = list(annotation_stage_id.keys())\n",
    "    stages_id_list = list(annotation_stage_id.values())\n",
    "\n",
    "    for iteration in range(1, len(times_events)):\n",
    "\n",
    "        start = int(times_events[iteration - 1][0] / sfreq)\n",
    "        duration = int(times_events[iteration][0] / sfreq) - start\n",
    "        stage_id = times_events[iteration - 1][2]\n",
    "\n",
    "        current_stage_name = stages_names_list[stages_id_list.index(stage_id)]\n",
    "\n",
    "        x.append(start)\n",
    "        y.append(current_stage_name)\n",
    "        x.append(start + duration)\n",
    "        y.append(current_stage_name)\n",
    "\n",
    "        image_name = \"*\" + str(start) + \".png\"  # image name of the signal for specific PSG folder in the dataset_predict\n",
    "        possible_image = fnmatch.filter(os.listdir(img_folder_path), image_name)[0]\n",
    "\n",
    "        if not possible_image:\n",
    "            print(f\"Possible image file {image_name} hasn't been found.\")\n",
    "            continue\n",
    "\n",
    "        true_list[stage_id - 1] += 1  # counting stage elements\n",
    "\n",
    "        # print(f\"Image that is currently being processed: {possible_image}, start time: {start}\")\n",
    "\n",
    "        path_possible_img = img_folder_path + \"/\" + possible_image\n",
    "\n",
    "        expan_dim_image = image.load_img(path_possible_img, color_mode='grayscale', target_size=(800, 800))\n",
    "        expan_dim_image = image.img_to_array(expan_dim_image)\n",
    "        expan_dim_image = np.expand_dims(expan_dim_image, axis=0)\n",
    "        predict_res = cnn.predict(expan_dim_image)\n",
    "        \n",
    "        # print(f\"Predicted raw result: {predict_res}\")\n",
    "        # print(f\"Class indexes: {training_set.class_indices}\")\n",
    "\n",
    "        index_predict = list(predict_res[0])\n",
    "        # print(index_predict)\n",
    "\n",
    "        \n",
    "        predicted_value = np.argmax(index_predict)   # index\n",
    "\n",
    "        prediction_main_list = list(training_set.class_indices.keys())\n",
    "        prediction_id_main_list = list(training_set.class_indices.values())\n",
    "        \n",
    "        predicted_stage_name = prediction_main_list[predicted_value] # index of the current sleep stage in the prediction list\n",
    "        id_stage_predict = stages_names_list.index(predicted_stage_name) + 1\n",
    "        \n",
    "        # print(f\"Predicted value - {id_stage_predict}/{stage_id}\")\n",
    "\n",
    "        if id_stage_predict == stage_id:\n",
    "            correct_predict_list[stage_id-1] += 1\n",
    "\n",
    "        x_pred.append(start)\n",
    "        y_pred.append(predicted_stage_name)\n",
    "        x_pred.append(start + duration)\n",
    "        y_pred.append(predicted_stage_name)\n",
    "\n",
    "    print(\"======== Coordinates for true and predicted hypnogram have been received. ========\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Hypnogram plotting\n",
    "\n",
    "    stages_names_for_plot = {\n",
    "        \"Sleep stage 4\": 0,\n",
    "        \"Sleep stage 3\": 1,\n",
    "        \"Sleep stage 2\": 2,\n",
    "        \"Sleep stage 1\": 3,\n",
    "        \"Sleep stage R\": 4,        \n",
    "        \"Sleep stage W\": 5\n",
    "    }\n",
    "\n",
    "    positions_of_labels = [0, 1, 2, 3, 4, 5]\n",
    "    y_true_data_plot = [stages_names_for_plot[elem] for elem in y]\n",
    "    y_pred_data_plot = [stages_names_for_plot[elem] for elem in y_pred]\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 15\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(10)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, y_true_data_plot, '-b', linewidth='1', label=\"True hypnogram\")\n",
    "    plt.yticks(positions_of_labels, stages_names_for_plot)\n",
    "    plt.xticks([])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Sleep stage')\n",
    "    plt.title(f\"Hypnograms for {img_folder_path[-7:]}\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x_pred, y_pred_data_plot, '-g', linewidth='1', label=\"Predicted hypnogram\")\n",
    "    plt.yticks(positions_of_labels, stages_names_for_plot)\n",
    "    plt.xticks([])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Sleep stage')\n",
    "    plt.legend()\n",
    "\n",
    "    hypnogram_folder = img_folder_path + \"/\" + \"hypnogram_info\" + img_folder_path[-6:]\n",
    "    if not os.path.isdir(hypnogram_folder):\n",
    "        os.mkdir(hypnogram_folder)\n",
    "\n",
    "    path_save_img = hypnogram_folder + f\"/hypnogram_{img_folder_path[-6:]}\"\n",
    "    plt.savefig(path_save_img, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Creating confusion map\n",
    "\n",
    "    \n",
    "    for elem in y_true_data_plot:\n",
    "        elem += 1\n",
    "        \n",
    "    for elem in y_pred_data_plot:\n",
    "        elem += 1\n",
    "    \n",
    "    path_save_img = hypnogram_folder + f\"/confusion_map_{img_folder_path[-6:]}\"\n",
    "    map = create_confusion_matrix(y_true_data_plot, y_pred_data_plot, path_save_img)\n",
    "\n",
    "\n",
    "    # Percentage of predictions\n",
    "    try:\n",
    "        percent = round(sum(correct_predict_list)/sum(true_list) * 100, 2)\n",
    "    except ZeroDivisionError:\n",
    "        percent = 0\n",
    "\n",
    "    info = f\"All prediction results for {img_folder_path[-6:]}: {sum(correct_predict_list)}/{sum(true_list)} \\\n",
    "                    ({percent}%)\\n\"\n",
    "\n",
    "    path_for_txt = hypnogram_folder + f\"/predict_info_{img_folder_path[-6:]}.txt\"\n",
    "    with open(path_for_txt, \"w\") as file:\n",
    "        file.write(info)\n",
    "\n",
    "    print(info)\n",
    "\n",
    "    for iteration in range(len(correct_predict_list)):\n",
    "        try:\n",
    "            percent = round(correct_predict_list[iteration]/true_list[iteration] * 100, 2)\n",
    "        except ZeroDivisionError:\n",
    "            if correct_predict_list[iteration] == true_list[iteration]:\n",
    "                percent = 100\n",
    "            else:\n",
    "                percent = 0\n",
    "        info = f\"Prediction results for {stages_names_list[iteration]}: {correct_predict_list[iteration]}/{true_list[iteration]} \\\n",
    "                    ({percent}%)\\n\"\n",
    "        print(info)\n",
    "\n",
    "        with open(path_for_txt, \"a\") as file:\n",
    "            file.write(info)\n",
    "    \n",
    "    print(f\"======== Hypnogram {img_folder_path[-7:]} has been created. ========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51a1aa-a815-4000-9ce6-7db9b872174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_create_hypns(cnn, dir_edf_files_predict, training_set):\n",
    "\n",
    "    dir_predict_dataset = \"dataset_predict\"\n",
    "\n",
    "    if not os.path.isdir(dir_predict_dataset):\n",
    "        print(\"Prediction dataset hasn't been found. Impossible to make a prediction.\")\n",
    "        return\n",
    "\n",
    "    if not cnn:\n",
    "        cnn = tf.keras.models.load_model('models/exit_model.keras')\n",
    "        \n",
    "    print(f\"Summary for cnn model '{model_path}':\\n\")\n",
    "    cnn.summary()\n",
    "\n",
    "    for folder in os.listdir(dir_predict_dataset):\n",
    "\n",
    "        curr_path = dir_predict_dataset + \"/\" + folder\n",
    "\n",
    "        psg_file = folder + \"*\"  # name of the true psg signal files (psg and hyp) in the dir_edf_files_predict\n",
    "        possible_psg_files = fnmatch.filter(os.listdir(dir_edf_files_predict), psg_file)\n",
    "\n",
    "        psg_file_path = \"\"\n",
    "        hyp_file_path = \"\"\n",
    "        for file in possible_psg_files:\n",
    "            if file.split('-')[1][0] == \"H\":\n",
    "                hyp_file_path = dir_edf_files_predict + \"/\" + file\n",
    "            else:\n",
    "                psg_file_path = dir_edf_files_predict + \"/\" + file\n",
    "\n",
    "        data, annotations = file_keeper(psg_file_path, hyp_file_path)\n",
    "        sfreq = data.info.get('sfreq')\n",
    "        events_from_the_file, event_id_info, annotations_stage_id = crop_set_annotations(data, annotations)\n",
    "\n",
    "        tmax = 30.0 - 1.0 / data.info[\"sfreq\"]\n",
    "        epochs_from_the_file = mne.Epochs(\n",
    "            raw=data,\n",
    "            events=events_from_the_file,\n",
    "            event_id=event_id_info,\n",
    "            tmin=0.0,\n",
    "            tmax=tmax,\n",
    "            baseline=None,\n",
    "            verbose=False\n",
    "        )\n",
    "        events = epochs_from_the_file.get_data(picks=[0])\n",
    "        times_events = epochs_from_the_file.events\n",
    "\n",
    "        print(f\"Current path: {curr_path}\")\n",
    "        create_hypnograms(events, times_events, sfreq, curr_path, cnn, annotations_stage_id, training_set)\n",
    "\n",
    "    print(\"All hypnograms have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f29fb-347c-4922-a13d-a0e134953b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "def create_confusion_matrix(true_array, prediction_array, path_save_img=''):\n",
    "    labels_stages = [1, 2, 3, 4, 5, 6]\n",
    "    result = confusion_matrix(true_array, prediction_array, labels=labels_stages)\n",
    "    print(f\"\\nConfusion matrix:\\n{result}\\n\")\n",
    "\n",
    "    table = pd.DataFrame(result, range(len(labels_stages)), range(len(labels_stages)))\n",
    "    fig = plt.figure()\n",
    "    map = sn.heatmap(table, annot=True, fmt='g')\n",
    "\n",
    "    if path_save_img:\n",
    "        plt.savefig(path_save_img)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85467245-96cf-4dc8-bd34-620c2b20b63c",
   "metadata": {},
   "source": [
    "# EXIT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d957a6d-8f3a-4565-8600-0828d082f7ac",
   "metadata": {},
   "source": [
    "## CREATING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c95c6d-676f-450c-9648-704867737ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directories()\n",
    "\n",
    "print(\"\\n\\n======== CREATING TRAINING SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/sleep-cassette\"\n",
    "optimal_number_images, max_number_images = find_min_num_of_file(dir)\n",
    "dataset_create(dir, optimal_number_images, \"train\")\n",
    "\n",
    "print(\"\\n\\n======== CREATING TEST SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/x_test_y_test\"\n",
    "optimal_number_images, max_number_images = find_min_num_of_file(dir)\n",
    "dataset_create(dir, optimal_number_images, \"test\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n======== CREATING VALIDATION SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/x_valid_y_valid\"\n",
    "optimal_number_images, max_number_images = find_min_num_of_file(dir)\n",
    "dataset_create(dir, optimal_number_images, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912d519-d99e-4df6-b6df-ef37c699adb9",
   "metadata": {},
   "source": [
    "## CREATING PREDICTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19989f7c-2092-46dd-8824-5689cd2a2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_directory = \"edf_files/x_test_y_test\"\n",
    "create_predict_dataset(files_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9430e-3278-4fbc-a060-224f483746ee",
   "metadata": {},
   "source": [
    "## Training CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e5a85-1ffc-407a-a2c7-2e21cde97b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_hid_files()\n",
    "training_set, validation_set, test_set = preprocessing_func()\n",
    "cnn = build_cnn()\n",
    "cnn_train_model(cnn, training_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffe7e6-fc8e-4f75-a5a0-413a0c97426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, acc_value = cnn.evaluate(test_set)\n",
    "\n",
    "print(f\"Loss value: {loss_value}\")\n",
    "print(f\"Accuracy value: {acc_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab817306-63f5-4e75-a80a-e59203b3c76f",
   "metadata": {},
   "source": [
    "## PREDICTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36731fce-f036-4c8b-990f-ab329498a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_edf_files_predict = \"edf_files/x_test_y_test\"\n",
    "model_path = 'models/exit_model.keras'\n",
    "make_predict_create_hypns(_, dir_edf_files_predict, training_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
