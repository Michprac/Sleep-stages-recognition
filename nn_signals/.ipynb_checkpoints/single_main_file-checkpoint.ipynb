{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658c3f7e-e644-4711-a546-595eabd14097",
   "metadata": {},
   "source": [
    "## PREPARING TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9ddc94-9977-48c4-a0f2-f1bf121cf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os, fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159ee40d-61f6-493d-b6e4-892ae71dca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_keeper(psg_file_path, hyp_file_path):\n",
    "\n",
    "    data = mne.io.read_raw_edf(psg_file_path, stim_channel=\"Event marker\", infer_types=True, preload=True)\n",
    "    annotations = mne.read_annotations(hyp_file_path)\n",
    "\n",
    "    data.filter(0.5, 30, picks=[0, 1, 2])\n",
    "    data.filter(None, 5, picks=[4])\n",
    "    \n",
    "\n",
    "    return data, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0727d5-061d-4779-b328-a99761df4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_set_annotations(data, annotations):\n",
    "\n",
    "    annotations.crop(annotations[1][\"onset\"] - 30*60, annotations[-2][\"onset\"] + 30*60)\n",
    "    data.set_annotations(annotations, emit_warning=False)\n",
    "\n",
    "    annotations_stage_id = {\n",
    "        \"Sleep stage W\": 1,\n",
    "        \"Sleep stage 1\": 2,\n",
    "        \"Sleep stage 2\": 3,\n",
    "        \"Sleep stage 3\": 4,\n",
    "        \"Sleep stage 4\": 5,\n",
    "        \"Sleep stage R\": 6,\n",
    "    }\n",
    "\n",
    "    events_from_the_file, event_id_info = mne.events_from_annotations(\n",
    "        data, event_id= annotations_stage_id, chunk_duration= 30.0\n",
    "    )\n",
    "\n",
    "    return events_from_the_file, event_id_info, annotations_stage_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1808a91-d7e8-4de0-8060-03297e70482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signals(data, events_from_the_file, annotations_id, psg_file, epochs_array):\n",
    "\n",
    "    tmax = 30.0 - 1.0 / data.info[\"sfreq\"]\n",
    "    print(f\"Number of epochs in events_from_the_file for the file: {len(events_from_the_file)}\")\n",
    "\n",
    "    epochs_from_the_file = mne.Epochs(\n",
    "        raw=data,\n",
    "        events=events_from_the_file,\n",
    "        event_id=annotations_id,\n",
    "        tmin=0.0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        verbose=False        \n",
    "    )\n",
    "    epochs_array.append(epochs_from_the_file)\n",
    "\n",
    "    print(f\"\\n  SIGNALS HAVE BEEN PREPARED FOR {psg_file.split(\"-\")[0][0:-2]}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7852c16-5830-475a-be9c-fea9df334cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_create(files_directory):\n",
    "\n",
    "    epochs_array = []\n",
    "  \n",
    "    all_files = os.listdir(files_directory)\n",
    "    not_processed_files = []\n",
    "    all_psg_files = []\n",
    "    all_hyp_files = []\n",
    "\n",
    "    for file in all_files:\n",
    "\n",
    "        if file[0] == '.': # for ignoring hidden files in the directory\n",
    "            continue\n",
    "    \n",
    "        parts = file.split(\"-\")\n",
    "    \n",
    "        if parts[1] == \"PSG.edf\":\n",
    "            all_psg_files.append(file)\n",
    "        elif parts[1] == \"Hypnogram.edf\":\n",
    "            all_hyp_files.append(file)\n",
    "\n",
    "    # for controlling status progress\n",
    "    old_process_percentage = 0\n",
    "    process_percentage = 0\n",
    "    iteration = 0\n",
    "\n",
    "    for psg_file in all_psg_files:\n",
    "\n",
    "        hyp_file = psg_file.split(\"-\")[0][:-2] + \"*\" + \"-Hypnogram.edf\"\n",
    "        possible_hyp = fnmatch.filter(all_hyp_files, hyp_file)\n",
    "\n",
    "        if possible_hyp:\n",
    "\n",
    "            hyp_file = possible_hyp[0]\n",
    "\n",
    "            print(f\"\\n================ Files currently being processed: {psg_file}, {hyp_file} ================\")\n",
    "\n",
    "            psg_file_path = files_directory + \"/\" + psg_file\n",
    "            hyp_file_path = files_directory + \"/\" + hyp_file\n",
    "\n",
    "            data, annotations = file_keeper(psg_file_path, hyp_file_path)\n",
    "            print(\"======== Got data and annotations. ========\")\n",
    "\n",
    "            events_from_the_file, annotations_id, _ = crop_set_annotations(data, annotations)\n",
    "            print(\"======== Annotations cropped and set. ========\")\n",
    "\n",
    "            create_signals(data, events_from_the_file, annotations_id, psg_file, epochs_array)\n",
    "            \n",
    "        else:\n",
    "            not_processed_file = psg_file.split(\"-\")[0][:-2]    # Get number of the candidate, i.e. SC4812\n",
    "            print(f\"No such hypnogram file for {not_processed_file}\")\n",
    "            not_processed_files.append(not_processed_file)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        process_percentage = round(iteration / len(all_psg_files) * 100)   # process status controlling\n",
    "\n",
    "        if process_percentage != old_process_percentage:\n",
    "            print(f\"======== Extracting signals data from PSG files: {process_percentage}% ========\\n\")\n",
    "        old_process_percentage = process_percentage\n",
    "\n",
    "    print(\"END. Arrays for the dataset have been prepared.\")\n",
    "    if not_processed_files:\n",
    "        print(f\"Files that weren't processed: {not_processed_files}\")\n",
    "\n",
    "    return epochs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a34a8-808a-4e82-8e53-9206940c63e7",
   "metadata": {},
   "source": [
    "## PREPARING PREDICING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d71b40-9b6d-46bc-9490-e904f5a9a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signals_predict(data, events_from_the_file, annotations_id, psg_file):\n",
    "\n",
    "    epochs_array = []\n",
    "    \n",
    "    tmax = 30.0 - 1.0 / data.info[\"sfreq\"]\n",
    "    print(f\"Number of epochs in events_from_the_file for the file: {len(events_from_the_file)}\")\n",
    "\n",
    "    epochs_from_the_file = mne.Epochs(\n",
    "        raw=data,\n",
    "        events=events_from_the_file,\n",
    "        event_id=annotations_id,\n",
    "        tmin=0.0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        verbose=False        \n",
    "    )\n",
    "    epochs_array.append(epochs_from_the_file)\n",
    "\n",
    "    print(f\"\\n  SIGNALS HAVE BEEN PREPARED FOR {psg_file}.\\n\")\n",
    "    \n",
    "    return epochs_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e74092f-e13d-44d2-8ce1-47b96f91076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(files_names):\n",
    "\n",
    "    directories = ()\n",
    "    dir_name = \"info_output\"\n",
    "\n",
    "    if not os.path.isdir(dir_name):\n",
    "\n",
    "        main_dir = dir_name\n",
    "        test_files_dir = []\n",
    "\n",
    "        for i in files_names:\n",
    "            test_files_dir.append(main_dir + \"/\" + i)\n",
    "\n",
    "        directories = (main_dir, test_files_dir)\n",
    "\n",
    "        for directory in directories:\n",
    "            if isinstance(directory, list):\n",
    "                for sub_directory in directory:\n",
    "                    if os.path.isdir(sub_directory):\n",
    "                        print(f\"Folder '{sub_directory}' already exists.\")\n",
    "                    else:\n",
    "                        os.mkdir(sub_directory)\n",
    "            else:\n",
    "                if os.path.isdir(directory):\n",
    "                    print(f\"Folder '{directory}' already exists.\")\n",
    "                else:\n",
    "                    os.mkdir(directory)\n",
    "\n",
    "        print(\"\\n======== Directories has been created. ========\\n\")\n",
    "\n",
    "        return main_dir\n",
    "    else:\n",
    "        print(\"Prediction output directory already exists.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566f7bd7-c6e8-42df-86c1-6b89b709aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_make(x_list, y_list, model):   # x_list, y_list i.e. (5282, 43), (5282)\n",
    "\n",
    "    print(\"Prediction process has been started\")\n",
    "    print(f\"SHape of the features array: {x_list.shape}\")\n",
    "    print(f\"SHape of the labels array: {y_list.shape}\")\n",
    "\n",
    "    correct = 0\n",
    "    all_elements = x_list.shape[0]\n",
    "    prediction_array = []\n",
    "    true_array = []\n",
    "\n",
    "    progress = 0\n",
    "\n",
    "    predictions_for_epochs = model.predict(x_list)\n",
    "\n",
    "    for element in range(all_elements):\n",
    "\n",
    "        prediction_for_epoch = np.argmax(predictions_for_epochs[element]) + 1   # Class id, for example 1 for \"Sleep stage W\"\n",
    "        prediction_array.append(prediction_for_epoch)\n",
    "\n",
    "        true_result = y_list[element] + 1     # Class id, for example 1 for \"Sleep stage W\"\n",
    "        true_array.append(true_result)\n",
    "\n",
    "        if prediction_for_epoch == true_result:\n",
    "            correct += 1\n",
    "\n",
    "        progress = round(element/all_elements * 100, 2)\n",
    "        print(f\"Progress of the prediction process: {progress}%\", end=\"\\r\")\n",
    "\n",
    "    percentage = round(correct/all_elements*100, 2)\n",
    "    print(f\"\\nFinal result is: {correct} / {all_elements} ({percentage}%)\")\n",
    "\n",
    "    return correct, all_elements, prediction_array, true_array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20138791-383d-4ecb-b718-1783325f403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hypnograms(features_data, labels_data, epochs_array, sfreq, img_folder_path, model, annotations_stage_id):\n",
    "\n",
    "    # Coordinates for true hypnogram and predicted respectively\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    x_pred = []\n",
    "    y_pred = []\n",
    "\n",
    "    percent = 0    # Variable for an exit text statistical information\n",
    "    true_list = 6*[0]   # List for true numbers of the each stage, based on the annotation_stage_id\n",
    "    correct_predict_list = 6*[0]   # List of the correct predictions for the each stage, based on the annotation_stage_id\n",
    "\n",
    "    stages_names_list = list(annotations_stage_id.keys())\n",
    "    stages_id_list = list(annotations_stage_id.values())\n",
    "\n",
    "    _, _, predictions, true_array = prediction_make(features_data, labels_data, model)\n",
    "\n",
    "    true_events = epochs_array[0].events\n",
    "\n",
    "    for iteration in range(1, len(true_events)):\n",
    "\n",
    "        start = int(true_events[iteration - 1][0] / sfreq)\n",
    "        duration = int(true_events[iteration][0] / sfreq) - start\n",
    "        stage_id = true_events[iteration - 1][2]\n",
    "\n",
    "        current_stage_name = stages_names_list[stages_id_list.index(stage_id)]\n",
    "\n",
    "        x.append(start)\n",
    "        y.append(current_stage_name)\n",
    "        x.append(start + duration)\n",
    "        y.append(current_stage_name)\n",
    "\n",
    "        true_list[stages_id_list.index(stage_id)] += 1  # counting stage elements\n",
    "\n",
    "\n",
    "        # Predictions results\n",
    "\n",
    "        predict_res = predictions[iteration - 1]   # Predicted value is a class id in the range [1, 6]\n",
    "\n",
    "        if predict_res == stage_id:\n",
    "            correct_predict_list[stages_id_list.index(predict_res)] += 1\n",
    "\n",
    "        # print(f\"Predicted value of stage id and true value - {predict_res}/{stage_id}\\n\\n\")\n",
    "\n",
    "        predicted_stage_name = stages_names_list[stages_id_list.index(predict_res)]\n",
    "\n",
    "        x_pred.append(start)\n",
    "        y_pred.append(predicted_stage_name)\n",
    "        x_pred.append(start + duration)\n",
    "        y_pred.append(predicted_stage_name)\n",
    "\n",
    "    print(\"======== Coordinates for true and predicted hypnograms have been received. ========\")\n",
    "\n",
    "    # Hypnogram plotting\n",
    "\n",
    "    stages_names_for_plot = {\n",
    "        \"Sleep stage 4\": 0,\n",
    "        \"Sleep stage 3\": 1,\n",
    "        \"Sleep stage 2\": 2,\n",
    "        \"Sleep stage 1\": 3,\n",
    "        \"Sleep stage R\": 4,        \n",
    "        \"Sleep stage W\": 5\n",
    "    }\n",
    "    positions_of_labels = [0, 1, 2, 3, 4, 5]\n",
    "    y_true_data_plot = [stages_names_for_plot[elem] for elem in y]\n",
    "    y_pred_data_plot = [stages_names_for_plot[elem] for elem in y_pred]\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 15\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(10)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, y_true_data_plot, '-b', linewidth='1', label=\"True hypnogram\")\n",
    "    plt.yticks(positions_of_labels, stages_names_for_plot)\n",
    "    plt.title(f\"Hypnogram for {img_folder_path[-7:]}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x_pred, y_pred_data_plot, '-g', linewidth='1', label=\"Predicted hypnogram\")\n",
    "    plt.yticks(positions_of_labels, stages_names_for_plot)\n",
    "    plt.legend()\n",
    "\n",
    "    hypnogram_folder = img_folder_path + \"/\" + \"hypnogram_info_\" + img_folder_path[-7:]\n",
    "    if not os.path.isdir(hypnogram_folder):\n",
    "        os.mkdir(hypnogram_folder)\n",
    "\n",
    "    path_save_img = hypnogram_folder + f\"/hypnogram_{img_folder_path[-7:]}\"\n",
    "    plt.savefig(path_save_img)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "    # Creating confusion map\n",
    "    path_save_img = hypnogram_folder + f\"/confusion_map_{img_folder_path[-7:]}\"\n",
    "    map = create_confusion_matrix(true_array, predictions, path_save_img)\n",
    "\n",
    "    # Percentage of predictions\n",
    "    try:\n",
    "        percent = round(sum(correct_predict_list)/sum(true_list) * 100, 2)\n",
    "    except ZeroDivisionError:\n",
    "        percent = 0\n",
    "\n",
    "    info = f\"All prediction results for {img_folder_path[-7:]}: {sum(correct_predict_list)}/{sum(true_list)} \\\n",
    "                ({percent}%)\\n\"\n",
    "    path_for_txt = hypnogram_folder + f\"/predict_info_{img_folder_path[-7:]}.txt\"\n",
    "    with open(path_for_txt, \"w\") as file:\n",
    "        file.write(info)\n",
    "\n",
    "    print(info)\n",
    "\n",
    "    for iteration in range(len(correct_predict_list)):\n",
    "        try:\n",
    "            percent = round(correct_predict_list[iteration]/true_list[iteration] * 100, 2)\n",
    "        except ZeroDivisionError:\n",
    "            percent = 0\n",
    "        info = f\"Prediction results for {stages_names_list[iteration]}: {correct_predict_list[iteration]}/{true_list[iteration]} \\\n",
    "                    ({percent}%)\\n\"\n",
    "        print(info)\n",
    "\n",
    "        with open(path_for_txt, \"a\") as file:\n",
    "            file.write(info)\n",
    "\n",
    "    print(f\"======== Hypnogram {img_folder_path[-7:]} has been created. ========\\n\\n\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491672bb-d062-4c16-8109-852b47a335bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_create_hypns(model, dir_edf_files_predict):\n",
    "\n",
    "    print(f\"Summary for the NN model:\\n\")\n",
    "    model.summary()\n",
    "\n",
    "    # Creating directory for predictions\n",
    "\n",
    "    files_for_predict = []\n",
    "    for file in os.listdir(dir_edf_files_predict):\n",
    "        name = file[0:7]\n",
    "        if name not in files_for_predict:\n",
    "            files_for_predict.append(name)\n",
    "\n",
    "    dir_predict_out = create_directories(files_for_predict)\n",
    "\n",
    "    if not dir_predict_out:\n",
    "        print(\"Please, remove old output directory!\")\n",
    "        return\n",
    "\n",
    "    # Creating hypnograms and confusion matrixes\n",
    "\n",
    "    exit_strings = []\n",
    "    \n",
    "    for folder in os.listdir(dir_predict_out):\n",
    "\n",
    "        curr_path = dir_predict_out + \"/\" + folder\n",
    "\n",
    "        psg_file = folder + \"*\"   # name of the true psg signal files (psg and hyp) in the dir_edf_files_predict\n",
    "        possible_psg_files = fnmatch.filter(os.listdir(dir_edf_files_predict), psg_file)\n",
    "\n",
    "        psg_file_path = \"\"\n",
    "        hyp_file_path = \"\"\n",
    "        for file in possible_psg_files:\n",
    "            if file.split('-')[1][0] == \"H\":\n",
    "                hyp_file_path = dir_edf_files_predict + \"/\" + file\n",
    "            else:\n",
    "                psg_file_path = dir_edf_files_predict + \"/\" + file\n",
    "\n",
    "        data, annotations = file_keeper(psg_file_path, hyp_file_path)\n",
    "        sfreq = data.info[\"sfreq\"]\n",
    "        events_from_the_file, annotations_id, annotations_stage_id = crop_set_annotations(data, annotations)\n",
    "\n",
    "        print(f\"Current path: {curr_path}\")\n",
    "\n",
    "        epochs_array = create_signals_predict(data, events_from_the_file, annotations_id, folder)\n",
    "        features_data, labels_data = feature_extract(epochs_array)\n",
    "        create_hypnograms(features_data, labels_data, epochs_array, sfreq, curr_path, model, annotations_stage_id)\n",
    "\n",
    "    print(\"End. All hypnograms have been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e764779-7f53-4c63-899e-3071a4a97876",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac7f5120-0d0b-4146-8f3a-c9749ce82e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy.fft as fft\n",
    "\n",
    "# features functions for a feature extraction\n",
    "\n",
    "def k_complex_ratio(signal):\n",
    "\n",
    "    signal = signal * 1000000    # Convert to the microvolts\n",
    "\n",
    "    sampling_freq = 100\n",
    "    min_complex_amplitude = 75\n",
    "    complex_duration = 0.5\n",
    "    k_complex_count = 0\n",
    "\n",
    "    duration_samples = int(complex_duration * sampling_freq)\n",
    "\n",
    "    for i in range(len(signal) - duration_samples):\n",
    "        segment = signal[i:i + duration_samples]\n",
    "        if np.max(segment) - np.min(segment) > min_complex_amplitude:\n",
    "            k_complex_count += 1\n",
    "\n",
    "    total_samples = len(signal)\n",
    "    ratio = k_complex_count / total_samples\n",
    "\n",
    "    return ratio\n",
    "\n",
    "def feature_mean(signal):\n",
    "    mean = np.mean(signal)\n",
    "    return mean\n",
    "\n",
    "def feature_variance(signal):\n",
    "    var = np.var(signal)\n",
    "    return var\n",
    "\n",
    "def feature_std_deviation(signal):\n",
    "    deviation = np.std(signal)\n",
    "    return deviation\n",
    "\n",
    "def feature_max(signal):\n",
    "    maximum = np.max(signal)\n",
    "    return maximum\n",
    "\n",
    "def feature_min(signal):\n",
    "    minimum = np.min(signal)\n",
    "    return minimum\n",
    "\n",
    "def feature_pkp(signal):\n",
    "    pkp = np.max(signal) - np.min(signal)\n",
    "    return pkp\n",
    "\n",
    "def feature_discrete_diff(signal):  # substraction from the following value the previous one\n",
    "    diff = np.sum(np.abs(np.diff(signal)))\n",
    "    return diff\n",
    "\n",
    "def feature_skewness(signal):  # how values are laying relatively to the normal symmetrical distribution\n",
    "    skew = stats.skew(signal, axis=-1)\n",
    "    return skew\n",
    "    \n",
    "def feature_kurtosis(signal):  # how values are laying relatively to the mean data\n",
    "    kurtosis = stats.kurtosis(signal, axis=-1)\n",
    "    return kurtosis\n",
    "\n",
    "\n",
    "\n",
    "def feature_freq_info(signal):\n",
    "\n",
    "    freq_info = []\n",
    "\n",
    "    sampling_freq = 100\n",
    "    fft_output = np.abs(np.fft.rfft(signal))\n",
    "    fft_freq = np.fft.rfftfreq(len(signal), 1/sampling_freq)\n",
    "\n",
    "    delta = np.sum(fft_output[(fft_freq >= 1) & (fft_freq < 4)])   # Frequencies are in Hz\n",
    "    freq_info.append(delta)\n",
    "    theta = np.sum(fft_output[(fft_freq >= 4) & (fft_freq <= 8)])\n",
    "    freq_info.append(theta)\n",
    "    alpha = np.sum(fft_output[(fft_freq > 8) & (fft_freq <= 13)])\n",
    "    freq_info.append(alpha)\n",
    "    beta = np.sum(fft_output[(fft_freq > 13) & (fft_freq <= 30)])\n",
    "    freq_info.append(beta)\n",
    "\n",
    "    return freq_info\n",
    "\n",
    "def feature_eye_blink(signal):\n",
    "\n",
    "    sampling_freq = 100\n",
    "\n",
    "    signal = signal * 1000000    # Convert to the microvolts\n",
    "\n",
    "    blink_ampl = 380  # in microvolts\n",
    "    blink_duration = 0.11 * sampling_freq\n",
    "    blinks = np.where(signal > blink_ampl)[0]\n",
    "\n",
    "    blink_times = np.diff(blinks) > blink_duration\n",
    "    correct_blinks = np.sum(blink_times) + 1\n",
    "\n",
    "    blink_frequency = correct_blinks / len(signal)\n",
    "\n",
    "    return blink_frequency\n",
    "\n",
    "def feature_median_of_freq(signal):\n",
    "\n",
    "    sampling_freq = 100\n",
    "    fft_output = np.abs(np.fft.rfft(signal))\n",
    "    fft_freq = np.fft.rfftfreq(len(signal), 1/sampling_freq)\n",
    "\n",
    "    important_freqs_start_from = 0.5 * np.max(fft_output)\n",
    "\n",
    "    median = np.median(fft_freq[fft_output >= important_freqs_start_from])\n",
    "\n",
    "    return median\n",
    "\n",
    "def feature_rms(signal):\n",
    "    rms = np.sqrt( np.sum(np.square(signal))/len(signal) )\n",
    "    return rms\n",
    "\n",
    "\n",
    "def features_from_eeg(eeg):\n",
    "\n",
    "    eeg_features = []\n",
    "\n",
    "    eeg_features.append(feature_max(eeg))\n",
    "    eeg_features.append(feature_min(eeg))\n",
    "    eeg_features.append(feature_mean(eeg))\n",
    "    eeg_features.append(feature_variance(eeg))\n",
    "    eeg_features.append(feature_std_deviation(eeg))\n",
    "    eeg_features.append(feature_pkp(eeg))\n",
    "    eeg_features.append(feature_discrete_diff(eeg))\n",
    "    eeg_features.append(feature_skewness(eeg))\n",
    "    eeg_features.append(feature_kurtosis(eeg))\n",
    "\n",
    "    for feature in feature_freq_info(eeg):\n",
    "        eeg_features.append(feature)\n",
    "    eeg_features.append(k_complex_ratio(eeg))\n",
    "\n",
    "    return eeg_features\n",
    "\n",
    "def features_from_eog(eog):\n",
    "\n",
    "    eog_features = []\n",
    "\n",
    "    eog_features.append(feature_max(eog))\n",
    "    eog_features.append(feature_min(eog))\n",
    "    eog_features.append(feature_mean(eog))\n",
    "    eog_features.append(feature_variance(eog))\n",
    "    eog_features.append(feature_std_deviation(eog))\n",
    "    eog_features.append(feature_pkp(eog))    \n",
    "    eog_features.append(feature_eye_blink(eog))    \n",
    "\n",
    "    return eog_features\n",
    "\n",
    "def features_from_emg(emg):\n",
    "\n",
    "    emg_features = []\n",
    "\n",
    "    emg_features.append(feature_max(emg))\n",
    "    emg_features.append(feature_min(emg))\n",
    "    emg_features.append(feature_mean(emg))\n",
    "    emg_features.append(feature_median_of_freq(emg))\n",
    "    emg_features.append(feature_variance(emg))\n",
    "    emg_features.append(feature_std_deviation(emg))\n",
    "    emg_features.append(feature_rms(emg))\n",
    "    emg_features.append(feature_pkp(emg))\n",
    "\n",
    "    return emg_features\n",
    "\n",
    "\n",
    "# x[0,1,2,3] :   0 - eeg, 1 - eeg, 2 - eog, 3 - emg\n",
    "\n",
    "def all_features(x):\n",
    "    return np.concatenate( (features_from_eeg(x[0]), features_from_eeg(x[1]), \n",
    "                            features_from_eog(x[2]), features_from_emg(x[3])), axis=-1 )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba80617-9941-4383-ae34-1f2a0bb8f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(epochs_array_per_file):\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    iteration = 0\n",
    "    file_num = 0\n",
    "    for file in epochs_array_per_file:\n",
    "        class_id = file.events[:, 2]\n",
    "        signals_per_epoch = file.get_data(picks=[0,1,2,4])     # 0 - eeg, 1 - eeg, 2 - eog, 4 - emg\n",
    "        features_subarray = []\n",
    "        for all_signals in signals_per_epoch:\n",
    "            features_subarray.append(all_features(all_signals))\n",
    "        features.append(features_subarray)\n",
    "        labels.append(class_id)\n",
    "    \n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    for i in range(len(labels)):     # to transform classes id's to the range [0, 5] \n",
    "        labels[i] -= 1\n",
    "        \n",
    "    print(\"\\n Feature extraction process has been done.\")\n",
    "\n",
    "    return features, labels   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993ef19-1942-4094-91bd-d08c7b8bb776",
   "metadata": {},
   "source": [
    "## TRAINING MODEL PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3c4769-fb8b-4777-86d9-2f5062d84758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d38fdbf1-2ecf-4e7a-b638-38025a465b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mlp(x_train, y_train, x_test, y_test, x_val, y_val):\n",
    "\n",
    "    print(f\"Train dataset shape: X {x_train.shape}, Y {y_train.shape}\")\n",
    "    print(f\"Test dataset shape: X {x_test.shape}, Y {y_test.shape}\")\n",
    "    print(f\"Validation dataset shape: X {x_val.shape}, Y {y_val.shape}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(x_train.shape[1],)))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "    loss_value, acc_value = model.evaluate(x_test, y_test)\n",
    "\n",
    "    print(f\"Loss value: {loss_value}\")\n",
    "    print(f\"Accuracy value: {acc_value}\")\n",
    "    \n",
    "    print(\"\\nModel has been trained.\")\n",
    "    model.save('model_mlp_end.keras')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c212876-bafb-45f1-97a9-cdeea67b0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(true_array, prediction_array, path_save_img=''):\n",
    "    labels_stages = [1, 2, 3, 4, 5, 6]\n",
    "    result = confusion_matrix(true_array, prediction_array, labels=labels_stages)\n",
    "    print(result)\n",
    "\n",
    "    table = pd.DataFrame(result, range(len(labels_stages)), range(len(labels_stages)))\n",
    "    fig = plt.figure()\n",
    "    map = sn.heatmap(table, annot=True, fmt='g')\n",
    "\n",
    "    if path_save_img:\n",
    "        plt.savefig(path_save_img)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741bf2b-7afe-48af-b295-fdd9f633b45e",
   "metadata": {},
   "source": [
    "## EXIT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430ac86-f36e-4464-b3a1-1ac09ca75b69",
   "metadata": {},
   "source": [
    "Training dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f232f-3a79-4108-8265-cb6058162fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n======== CREATING TRAINING SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/sleep-cassette\"\n",
    "epochs_array_train = dataset_create(dir)\n",
    "\n",
    "print(\"\\n\\n======== CREATING TEST SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/x_test_y_test\"\n",
    "epochs_array_test = dataset_create(dir)\n",
    "\n",
    "print(\"\\n\\n======== CREATING VALIDATION SIGNALS ========\\n\\n\")\n",
    "dir = \"edf_files/x_valid_y_valid\"\n",
    "epochs_array_valid = dataset_create(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9a82f-d640-4245-b940-f8776f5aa4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, labels_train = feature_extract(epochs_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af24f6-9840-412b-9d3a-f80e1f867332",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test, labels_test = feature_extract(epochs_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa2614-7449-42e9-b4be-d415a8b25f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_val, labels_val = feature_extract(epochs_array_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8600b7c-2d56-4723-b8eb-8e2fc34079cb",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6190171-cf6a-4988-a871-e175f54b162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model_mlp(features_train, labels_train, features_test, labels_test, features_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a0cd5-e1cc-41c7-b869-eee5186ac003",
   "metadata": {},
   "source": [
    "Prediction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38898d88-808d-46f5-9ec2-3080163cb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_edf_files_predict = \"edf_files/x_test_y_test\"\n",
    "make_predict_create_hypns(model, dir_edf_files_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
